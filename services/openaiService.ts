// Service OpenAI pour les conversations IA
// ‚ö†Ô∏è IMPORTANT: Ajoutez votre cl√© OpenAI dans le fichier .env
// VITE_OPENAI_API_KEY=sk-proj-votre-cl√©-ici
const OPENAI_API_KEY = process.env.EXPO_PUBLIC_OPENAI_API_KEY || "VOTRE_CLE_OPENAI_ICI";

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface StreamingCallbacks {
  onChunk?: (chunk: string) => void;
  onComplete?: (fullResponse: string) => void;
  onError?: (error: Error) => void;
}

// Service pour envoyer des messages √† OpenAI (mode normal - pour fallback seulement)
export async function sendMessageToOpenAI(
  messages: ChatMessage[],
  model: string = 'gpt-4'
): Promise<string> {
  const API_KEY = process.env.EXPO_PUBLIC_OPENAI_API_KEY;
  
  if (!API_KEY) {
    throw new Error('Cl√© API OpenAI non configur√©e');
  }

  try {
    console.log('ü§ñ Envoi √† OpenAI:', { model, messagesCount: messages.length });
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: model,
        messages: messages,
        max_tokens: 1000,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error('‚ùå Erreur OpenAI:', response.status, errorData);
      throw new Error(`Erreur API OpenAI: ${response.status}`);
    }

    const data = await response.json();
    const assistantMessage = data.choices[0]?.message?.content || 'D√©sol√©, je n\'ai pas pu g√©n√©rer de r√©ponse.';
    
    console.log('‚úÖ R√©ponse OpenAI re√ßue');
    return assistantMessage;

  } catch (error: any) {
    console.error('‚ùå Erreur lors de l\'appel OpenAI:', error);
    throw new Error(error.message || 'Erreur de connexion √† OpenAI');
  }
}

// NOUVEAU : Service de streaming ultra-rapide avec XMLHttpRequest
export async function sendMessageToOpenAIStreaming(
  messages: ChatMessage[],
  callbacks: StreamingCallbacks,
  model: string = 'gpt-4',
  abortController?: AbortController
): Promise<void> {
  const API_KEY = process.env.EXPO_PUBLIC_OPENAI_API_KEY;
  
  if (!API_KEY) {
    callbacks.onError?.(new Error('Cl√© API OpenAI non configur√©e'));
    return;
  }

  console.log('üöÄ Streaming instantan√© avec XMLHttpRequest:', { model, messagesCount: messages.length });

  return new Promise<void>((resolve, reject) => {
    const xhr = new XMLHttpRequest();
    let fullResponse = '';
    let buffer = '';
    
    // Configuration de la requ√™te
    xhr.open('POST', 'https://api.openai.com/v1/chat/completions');
    xhr.setRequestHeader('Authorization', `Bearer ${API_KEY}`);
    xhr.setRequestHeader('Content-Type', 'application/json');
    
    // Gestion de l'abort
    if (abortController) {
      abortController.signal.addEventListener('abort', () => {
        console.log('‚èπÔ∏è Requ√™te XHR annul√©e');
        xhr.abort();
      });
    }

    // Traitement en temps r√©el des donn√©es re√ßues
    xhr.onreadystatechange = () => {
      if (xhr.readyState === XMLHttpRequest.LOADING || xhr.readyState === XMLHttpRequest.DONE) {
        // R√©cup√©rer seulement les nouvelles donn√©es
        const newData = xhr.responseText.slice(buffer.length);
        buffer = xhr.responseText;
        
        if (newData) {
          // ‚ùå SUPPRIM√â : Log verbeux
          // console.log('üì¶ Nouveau chunk re√ßu:', newData.length + ' caract√®res');
          processStreamChunk(newData, (content) => {
            fullResponse += content;
            callbacks.onChunk?.(content); // STREAMING INSTANTAN√â !
          });
        }
      }
      
      if (xhr.readyState === XMLHttpRequest.DONE) {
        if (xhr.status === 200) {
          console.log('‚úÖ Streaming termin√©:', fullResponse.length + ' caract√®res');
          callbacks.onComplete?.(fullResponse);
          resolve();
        } else {
          console.error('‚ùå Erreur XHR:', xhr.status, xhr.statusText);
          callbacks.onError?.(new Error(`Erreur API: ${xhr.status}`));
          reject(new Error(`Erreur API: ${xhr.status}`));
        }
      }
    };

    xhr.onerror = () => {
      console.error('‚ùå Erreur r√©seau XHR');
      callbacks.onError?.(new Error('Erreur de connexion'));
      reject(new Error('Erreur de connexion'));
    };

    xhr.onabort = () => {
      console.log('‚èπÔ∏è Requ√™te annul√©e');
      resolve(); // Pas d'erreur, juste annul√©
    };

    // Envoyer la requ√™te
    const requestBody = JSON.stringify({
      model: model,
      messages: messages,
      max_tokens: 1000,
      temperature: 0.7,
      stream: true, // CRUCIAL : Activer le streaming
    });

    console.log('üì§ Envoi requ√™te XHR streaming...');
    xhr.send(requestBody);
  });
}

// Fonction pour traiter les chunks de streaming Server-Sent Events
function processStreamChunk(chunk: string, onContent: (content: string) => void): void {
  const lines = chunk.split('\n');
  
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = line.slice(6).trim();
      
      if (data === '[DONE]') {
        console.log('üèÅ Signal [DONE] re√ßu');
        continue;
      }
      
      if (!data) continue;
      
      try {
        const parsed = JSON.parse(data);
        const content = parsed.choices?.[0]?.delta?.content;
        
        if (content) {
          onContent(content); // IMM√âDIAT !
        }
      } catch (e) {
        // Ignorer les chunks JSON malform√©s silencieusement
        continue;
      }
    }
  }
} 