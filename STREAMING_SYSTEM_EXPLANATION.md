# ğŸš€ SYSTÃˆME DE STREAMING I-ACTIV - GUIDE TECHNIQUE POUR REACT NATIVE

## Vue d'ensemble
Dans l'application web, le streaming fonctionne parfaitement grÃ¢ce Ã  plusieurs mÃ©canismes coordonnÃ©s qui permettent d'afficher les rÃ©ponses mot par mot dÃ¨s rÃ©ception. Voici comment reproduire cette rapiditÃ© en React Native.

## 1. ğŸ”„ MÃ‰CANISME PRINCIPAL DE STREAMING

### A. Configuration de l'API OpenAI (CÃ´tÃ© service)

```javascript
// Configuration essentielle pour le streaming
const stream = await openai.chat.completions.create({
  model: "gpt-4.1-mini-2025-04-14",
  messages: messagesForAPI,
  temperature: 0.7,
  stream: true  // âœ… CRUCIAL : Active le streaming
}, { signal: currentController.signal });

// Traitement chunk par chunk
for await (const chunk of stream) {
  if (currentController?.signal.aborted) {
    throw new Error('RESPONSE_STOPPED');
  }
  
  const content = chunk.choices[0]?.delta?.content || '';
  if (content) {
    responseText += content;
    onPartialResponse(content); // âœ… IMMÃ‰DIAT : Envoi du fragment
  }
}
```

### B. Points clÃ©s pour React Native
- Utiliser `fetch` avec `ReadableStream` ou `XMLHttpRequest` pour Server-Sent Events
- ImplÃ©menter `AbortController` pour pouvoir arrÃªter le streaming
- Parser les chunks JSON ligne par ligne (format Server-Sent Events)

## 2. ğŸ“¡ SYSTÃˆME DE CALLBACK OPTIMISÃ‰

### A. Callback `onPartialResponse` (Web)
```javascript
// Dans ChatWindow.jsx - Ligne 881+
(partialResponse, isComplete = false) => {
  setMessages(prev => {
    const currentMessage = prev.find(msg => msg.id === assistantMessage.id);
    if (!currentMessage) return prev;
    
    if (isComplete) {
      // Remplacement complet pour la rÃ©ponse finale
      return prev.map(msg =>
        msg.id === assistantMessage.id
        ? { ...msg, content: partialResponse }
        : msg
      );
    }
    
    // âœ… ACCUMULATION : Ajout du nouveau fragment
    const updatedContent = currentMessage.content + partialResponse;
    return prev.map(msg =>
      msg.id === assistantMessage.id
      ? { ...msg, content: updatedContent }
      : msg
    );
  });
}
```

### B. Pour React Native
```javascript
// Ã‰quivalent React Native avec useState
const [messages, setMessages] = useState([]);

const onPartialResponse = (partialText, isComplete = false) => {
  setMessages(prevMessages => {
    const messageIndex = prevMessages.findIndex(msg => msg.id === assistantMessageId);
    if (messageIndex === -1) return prevMessages;
    
    const updatedMessages = [...prevMessages];
    if (isComplete) {
      updatedMessages[messageIndex].content = partialText;
    } else {
      // âœ… CRUCIAL : Accumulation immÃ©diate
      updatedMessages[messageIndex].content += partialText;
    }
    return updatedMessages;
  });
};
```

## 3. âš¡ SYSTÃˆME DE BUFFER INTELLIGENT

### A. Optimisation des performances (Web)
```javascript
// Dans responsesApi.js - Ligne 390+
let lastUpdateTime = Date.now();
let updateBuffer = '';
const MIN_UPDATE_INTERVAL = 150; // 150ms entre les mises Ã  jour
const MIN_BUFFER_SIZE = 10;      // 10 caractÃ¨res minimum
const MAX_BUFFER_SIZE = 100;     // 100 caractÃ¨res maximum

// Logique de buffer optimisÃ©e
if (chunk.choices && chunk.choices[0]?.delta?.content) {
  const partialContent = chunk.choices[0].delta.content || '';
  
  updateBuffer += partialContent;
  combinedText += partialContent;
  
  const now = Date.now();
  const timeDiff = now - lastUpdateTime;
  const bufferSize = updateBuffer.length;
  
  // âœ… SMART UPDATE : Conditions optimisÃ©es
  if (timeDiff > MIN_UPDATE_INTERVAL && bufferSize >= MIN_BUFFER_SIZE || 
      bufferSize >= MAX_BUFFER_SIZE) {
    onPartialUpdate(updateBuffer, false);
    updateBuffer = '';
    lastUpdateTime = now;
  }
}

// Envoyer le reste du buffer Ã  la fin
if (updateBuffer.length > 0) {
  onPartialUpdate(updateBuffer, false);
}
```

### B. ImplÃ©mentation React Native
```javascript
// Service de streaming pour React Native
class StreamingService {
  constructor() {
    this.buffer = '';
    this.lastUpdateTime = 0;
    this.MIN_UPDATE_INTERVAL = 100; // Plus rapide sur mobile
    this.MIN_BUFFER_SIZE = 5;       // Plus petit sur mobile
    this.MAX_BUFFER_SIZE = 50;      // Plus petit sur mobile
  }
  
  processChunk(chunk, onPartialUpdate) {
    this.buffer += chunk;
    
    const now = Date.now();
    const timeDiff = now - this.lastUpdateTime;
    const bufferSize = this.buffer.length;
    
    if (timeDiff > this.MIN_UPDATE_INTERVAL && bufferSize >= this.MIN_BUFFER_SIZE || 
        bufferSize >= this.MAX_BUFFER_SIZE) {
      onPartialUpdate(this.buffer);
      this.buffer = '';
      this.lastUpdateTime = now;
    }
  }
  
  flush(onPartialUpdate) {
    if (this.buffer.length > 0) {
      onPartialUpdate(this.buffer);
      this.buffer = '';
    }
  }
}
```

## 4. ğŸ”§ IMPLÃ‰MENTATION REACT NATIVE COMPLÃˆTE

### A. Service API avec Streaming
```javascript
// services/OpenAIStreamingService.js
export class OpenAIStreamingService {
  async streamChatCompletion(messages, onPartialResponse, abortController) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4.1-mini-2025-04-14',
        messages: messages,
        stream: true,
        temperature: 0.7
      }),
      signal: abortController?.signal
    });

    if (!response.body) {
      throw new Error('Streaming non supportÃ©');
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';
    let fullResponse = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        buffer += chunk;

        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') continue;

            try {
              const parsed = JSON.parse(data);
              const content = parsed.choices?.[0]?.delta?.content || '';
              if (content) {
                fullResponse += content;
                onPartialResponse(content); // âœ… IMMÃ‰DIAT
              }
            } catch (e) {
              console.warn('Erreur parsing JSON:', e);
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return fullResponse;
  }
}
```

### B. Hook React Native pour Chat
```javascript
// hooks/useChatStreaming.js
import { useState, useCallback, useRef } from 'react';

export const useChatStreaming = () => {
  const [messages, setMessages] = useState([]);
  const [isLoading, setIsLoading] = useState(false);
  const streamingService = useRef(new OpenAIStreamingService());
  const abortController = useRef(null);

  const sendMessage = useCallback(async (userMessage) => {
    // Ajouter le message utilisateur
    const userMsg = {
      id: Date.now(),
      type: 'user',
      content: userMessage,
      timestamp: new Date().toISOString()
    };

    // Ajouter message assistant vide
    const assistantMsg = {
      id: Date.now() + 1,
      type: 'assistant',
      content: '',
      timestamp: new Date().toISOString()
    };

    setMessages(prev => [...prev, userMsg, assistantMsg]);
    setIsLoading(true);

    abortController.current = new AbortController();

    try {
      await streamingService.current.streamChatCompletion(
        [...messages, userMsg].map(msg => ({
          role: msg.type === 'user' ? 'user' : 'assistant',
          content: msg.content
        })),
        // âœ… CALLBACK STREAMING
        (partialContent) => {
          setMessages(prevMessages => {
            const updatedMessages = [...prevMessages];
            const assistantIndex = updatedMessages.findIndex(
              msg => msg.id === assistantMsg.id
            );
            if (assistantIndex !== -1) {
              // âœ… ACCUMULATION IMMÃ‰DIATE
              updatedMessages[assistantIndex].content += partialContent;
            }
            return updatedMessages;
          });
        },
        abortController.current
      );
    } catch (error) {
      console.error('Erreur streaming:', error);
    } finally {
      setIsLoading(false);
    }
  }, [messages]);

  const stopGeneration = useCallback(() => {
    if (abortController.current) {
      abortController.current.abort();
    }
  }, []);

  return {
    messages,
    isLoading,
    sendMessage,
    stopGeneration
  };
};
```

### C. Composant Chat React Native
```javascript
// components/ChatScreen.js
import React from 'react';
import { View, Text, ScrollView, TextInput, TouchableOpacity } from 'react-native';
import { useChatStreaming } from '../hooks/useChatStreaming';

export const ChatScreen = () => {
  const { messages, isLoading, sendMessage, stopGeneration } = useChatStreaming();
  const [inputText, setInputText] = useState('');
  const scrollViewRef = useRef(null);

  // âœ… AUTO-SCROLL pendant le streaming
  useEffect(() => {
    if (scrollViewRef.current) {
      scrollViewRef.current.scrollToEnd({ animated: true });
    }
  }, [messages]);

  const handleSend = () => {
    if (inputText.trim()) {
      sendMessage(inputText.trim());
      setInputText('');
    }
  };

  return (
    <View style={{ flex: 1 }}>
      <ScrollView 
        ref={scrollViewRef}
        style={{ flex: 1 }}
        onContentSizeChange={() => 
          scrollViewRef.current?.scrollToEnd({ animated: true })
        }
      >
        {messages.map((message) => (
          <View key={message.id} style={{
            alignSelf: message.type === 'user' ? 'flex-end' : 'flex-start',
            backgroundColor: message.type === 'user' ? '#007AFF' : '#F0F0F0',
            padding: 10,
            borderRadius: 10,
            margin: 5,
            maxWidth: '80%'
          }}>
            <Text style={{
              color: message.type === 'user' ? 'white' : 'black'
            }}>
              {message.content}
            </Text>
          </View>
        ))}
      </ScrollView>
      
      <View style={{ flexDirection: 'row', padding: 10 }}>
        <TextInput
          style={{ flex: 1, borderWidth: 1, borderRadius: 5, padding: 10 }}
          value={inputText}
          onChangeText={setInputText}
          placeholder="Tapez votre message..."
          multiline
        />
        <TouchableOpacity 
          onPress={isLoading ? stopGeneration : handleSend}
          style={{ 
            backgroundColor: isLoading ? '#FF3B30' : '#007AFF',
            padding: 10,
            borderRadius: 5,
            marginLeft: 10 
          }}
        >
          <Text style={{ color: 'white' }}>
            {isLoading ? 'Stop' : 'Envoyer'}
          </Text>
        </TouchableOpacity>
      </View>
    </View>
  );
};
```

## 5. ğŸ¯ POINTS CRITIQUES POUR LA RAPIDITÃ‰

### A. Ce qui rend le streaming rapide dans l'app web :
1. **Pas d'attente** : `onPartialResponse(content)` appelÃ© immÃ©diatement
2. **Accumulation simple** : `currentMessage.content + partialResponse`
3. **Buffer intelligent** : Ã‰vite trop de re-renders
4. **Abort Controller** : Permet d'arrÃªter rapidement

### B. Optimisations spÃ©cifiques React Native :
1. **RÃ©duire les intervalles** : MIN_UPDATE_INTERVAL Ã  100ms au lieu de 150ms
2. **Utiliser `requestAnimationFrame`** pour les mises Ã  jour UI
3. **Ã‰viter les re-renders inutiles** avec `React.memo`
4. **Auto-scroll optimisÃ©** avec `onContentSizeChange`

## 6. ğŸš¨ ERREURS Ã€ Ã‰VITER

### âŒ Ne PAS faire :
```javascript
// MAUVAIS : Attendre la rÃ©ponse complÃ¨te
const response = await openai.chat.completions.create({
  stream: false  // âŒ Pas de streaming
});
setMessage(response.choices[0].message.content); // âŒ Tout d'un coup
```

### âœ… Ã€ faire :
```javascript
// BON : Streaming immÃ©diat
const stream = await openai.chat.completions.create({
  stream: true  // âœ… Streaming activÃ©
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || '';
  if (content) {
    onPartialResponse(content); // âœ… ImmÃ©diat
  }
}
```

## 7. ğŸ“± CONFIGURATION SPÃ‰CIFIQUE REACT NATIVE

### Package.json dependencies :
```json
{
  "dependencies": {
    "react-native": "^0.72.0",
    "react-native-fetch-api": "^3.0.0", // Pour streaming
    "react-native-url-polyfill": "^1.3.0" // Pour URL
  }
}
```

### Metro.config.js :
```javascript
// Pour supporter les streams
module.exports = {
  resolver: {
    platforms: ['ios', 'android', 'native', 'web'],
  },
  transformer: {
    getTransformOptions: async () => ({
      transform: {
        experimentalImportSupport: false,
        inlineRequires: true,
      },
    }),
  },
};
```

## 8. ğŸ¬ SÃ‰QUENCE COMPLÃˆTE

1. **Utilisateur tape message** â†’ Ajout immÃ©diat Ã  l'Ã©tat
2. **Message assistant vide crÃ©Ã©** â†’ Placeholder dans l'UI  
3. **Appel API streaming** â†’ `stream: true`
4. **Premier chunk reÃ§u** â†’ `onPartialResponse(chunk)` immÃ©diat
5. **Accumulation continue** â†’ `content += newChunk`
6. **Buffer intelligent** â†’ Optimise les re-renders
7. **Auto-scroll** â†’ Suit la gÃ©nÃ©ration
8. **Fin du stream** â†’ Message complet

## ğŸ’¡ RÃ‰SUMÃ‰ POUR L'IMPLÃ‰MENTATION

La clÃ© de la rapiditÃ© est :
1. **Streaming API activÃ©** (`stream: true`)
2. **Callback immÃ©diat** sans attente
3. **Accumulation simple** (`content += chunk`)
4. **Buffer optimisÃ©** pour les performances
5. **Auto-scroll** pendant la gÃ©nÃ©ration

Avec cette architecture, tu auras exactement la mÃªme rapiditÃ© qu'en web ! 